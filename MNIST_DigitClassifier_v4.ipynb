{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.ndimage.interpolation import shift\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Lambda, MaxPooling2D # convolution layers\n",
    "from keras.layers import Dense, Dropout, Flatten # core layers\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in the data. Label is the numeric label (0-9). Other columns in X data represent the pixel intensity (0-255) of the image\n",
    "# at the designated pixel. The MNIST data is already quite clean and well pre-processed, so we can just feed the data into\n",
    "# our model.\n",
    "\n",
    "train_data = pd.read_csv(\"mnist_train.csv\")\n",
    "X_train = train_data.drop(\"label\", axis = 1)\n",
    "y_train = train_data[\"label\"]\n",
    "\n",
    "test_data = pd.read_csv(\"mnist_test.csv\")\n",
    "X_test = test_data.drop(\"label\", axis = 1)\n",
    "y_test = test_data[\"label\"]\n",
    "\n",
    "X_train = X_train.values.reshape(60000,28,28,1)\n",
    "X_test = X_test.values.reshape(10000,28,28,1)\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 117s 62ms/step - loss: 0.1721 - accuracy: 0.9482 - val_loss: 0.0482 - val_accuracy: 0.9848\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 116s 62ms/step - loss: 0.0504 - accuracy: 0.9857 - val_loss: 0.0353 - val_accuracy: 0.9898\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 115s 61ms/step - loss: 0.0325 - accuracy: 0.9905 - val_loss: 0.0336 - val_accuracy: 0.9892\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 120s 64ms/step - loss: 0.0315 - accuracy: 0.9904 - val_loss: 0.0317 - val_accuracy: 0.9904\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 121s 65ms/step - loss: 0.0218 - accuracy: 0.9934 - val_loss: 0.0296 - val_accuracy: 0.9926\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 116s 62ms/step - loss: 0.0195 - accuracy: 0.9943 - val_loss: 0.0459 - val_accuracy: 0.9880\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 117s 62ms/step - loss: 0.0158 - accuracy: 0.9952 - val_loss: 0.0489 - val_accuracy: 0.9892\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 113s 60ms/step - loss: 0.0166 - accuracy: 0.9950 - val_loss: 0.0351 - val_accuracy: 0.9913\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 118s 63ms/step - loss: 0.0135 - accuracy: 0.9963 - val_loss: 0.0324 - val_accuracy: 0.9924\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 113s 60ms/step - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0228 - val_accuracy: 0.9945\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\", input_shape=(28,28,1)))\n",
    "model.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size = (3,3), activation=\"relu\"))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())    \n",
    "model.add(Conv2D(filters=256, kernel_size = (3,3), activation=\"relu\"))\n",
    "    \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512,activation=\"relu\"))\n",
    "    \n",
    "model.add(Dense(10,activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 10, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0228 - accuracy: 0.9945\n",
      "[0.022849183529615402, 0.9944999814033508]\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print (accuracy)\n",
    "\n",
    "y_test_vals = np.empty(len(y_test_pred), dtype = 'int32')\n",
    "for ix in range (0, len(y_test_pred)):\n",
    "    y_test_vals[ix] = np.argmax(y_test_pred[ix])\n",
    "\n",
    "print (y_test_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 978    0    0    0    0    0    1    1    0    0]\n",
      " [   0 1132    0    0    0    0    0    3    0    0]\n",
      " [   1    0 1026    1    0    0    0    4    0    0]\n",
      " [   0    0    0 1007    0    2    0    0    1    0]\n",
      " [   0    0    0    0  979    0    0    0    1    2]\n",
      " [   1    0    0    4    0  883    1    1    1    1]\n",
      " [   6    2    0    0    0    1  949    0    0    0]\n",
      " [   0    1    1    0    0    0    0 1023    0    3]\n",
      " [   0    0    1    1    0    1    0    0  970    1]\n",
      " [   0    0    0    0    7    1    0    1    2  998]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conf_matrix = tf.math.confusion_matrix(\n",
    "    y_test, y_test_vals, num_classes=10, weights=None, dtype=tf.dtypes.int32,\n",
    "    name=None\n",
    ")\n",
    "print (np.array(conf_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Evaluation:\n",
    "\n",
    "Model_v4 utilizes a Convolutional Neural Network. It yields a marginally better accuracy score on the test data (98.8% as opposed to model_v2's 97.1%).\n",
    "\n",
    "The Confusion Matrix for this model on the test data set (columns from 0-9, representing the classified digit):\n",
    "\n",
    "\n",
    "    [[ 973    0    3    0    0    0    2    1    1    0]\n",
    "     [   0 1127    3    0    2    2    0    1    0    0]\n",
    "     [   1    1 1023    0    1    0    0    5    1    0]\n",
    "     [   0    0    3 1000    0    4    0    2    1    0]\n",
    "     [   0    0    0    0  976    0    2    1    0    3]\n",
    "     [   1    0    0    5    0  882    3    0    1    0]\n",
    "     [   3    2    0    0    3    5  942    0    3    0]\n",
    "     [   0    2    6    0    1    0    0 1019    0    0]\n",
    "     [   2    0    3    1    6    0    2    2  954    4]\n",
    "     [   1    1    0    2   14    3    0    5    1  982]]\n",
    "\n",
    "As evidenced by the confusion matrix, model performance on 4's is the worst, often confusing them for 9's. This is understandable since 4's and 9's are visually similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: KNN_MNIST_ImageClassifier_v4\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save model to file called \"KNN_MNIST_ImageClassifier_v4\"\n",
    "model.save(\"KNN_MNIST_ImageClassifier_v4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-cpu",
   "language": "python",
   "name": "tf-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
